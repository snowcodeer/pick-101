# Lift task configuration
experiment:
  name: "lift_cube"
  base_dir: "runs"

training:
  timesteps: 500000
  eval_freq: 10000
  save_freq: 50000
  seed: 42

sac:
  learning_rate: 3.0e-4
  buffer_size: 100000
  learning_starts: 1000
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1

env:
  normalize_obs: true
  normalize_reward: true
  max_episode_steps: 200
  action_scale: 0.02      # 2cm per step max (Cartesian)
  lift_height: 0.08       # 8cm above ground
  hold_steps: 10          # Must hold for 10 steps
  reward_type: "dense"    # Dense rewards for faster learning
  reward_version: "v7"    # v1, v7 - see envs/lift_cube.py for details

eval:
  episodes: 10
  video_width: 640
  video_height: 480
